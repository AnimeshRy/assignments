from typing import List, Dict, Any, Optional
from app.services.embedding_service import EmbeddingService
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select
from app.models.document import Document, DocumentChunk
from app.models.qa_session import QASession, Question
import logging
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain.llms import FakeListLLM
import json

logger = logging.getLogger(__name__)

class RAGService:
    def __init__(self):
        """Initialize the RAG service."""
        self.embedding_service = EmbeddingService()
        logger.info("RAG service initialized")

        # Since we're using a mock LLM, we'll define some templates for responses
        self.qa_template = """
        Answer the question based on the following context:

        Context:
        {context}

        Question: {question}

        Answer:
        """

        # Initialize a fake LLM for demonstration purposes
        # In a real application, you would use a real LLM like OpenAI's GPT or a local model
        self.llm = FakeListLLM(
            responses=["I'll generate an answer based on the retrieved documents."]
        )

    async def get_qa_session_documents(self, session: AsyncSession, qa_session_id: int) -> List[Document]:
        """Get all documents associated with a QA session."""
        qa_session = await session.get(QASession, qa_session_id)
        if not qa_session:
            return []

        return qa_session.documents

    async def retrieve_relevant_chunks(
        self,
        session: AsyncSession,
        question: str,
        qa_session_id: int,
        top_k: int = 5
    ) -> List[Dict[str, Any]]:
        """Retrieve the most relevant document chunks for a question."""
        # Get documents from the QA session
        documents = await self.get_qa_session_documents(session, qa_session_id)
        if not documents:
            logger.warning(f"No documents found for QA session {qa_session_id}")
            return []

        # Get all chunks from these documents
        document_ids = [doc.id for doc in documents]
        stmt = select(DocumentChunk).where(DocumentChunk.document_id.in_(document_ids))
        result = await session.execute(stmt)
        chunks = result.scalars().all()

        if not chunks:
            logger.warning(f"No chunks found for documents {document_ids}")
            return []

        # Extract embeddings and content
        chunk_embeddings = [chunk.embedding for chunk in chunks if chunk.embedding]

        if not chunk_embeddings:
            logger.warning("No embeddings found in chunks")
            return []

        # Perform similarity search
        search_results = self.embedding_service.similarity_search(question, chunk_embeddings, top_k)

        # Map results back to chunks
        relevant_chunks = []
        for result in search_results:
            chunk_idx = result["index"]
            if chunk_idx < len(chunks):
                chunk = chunks[chunk_idx]
                relevant_chunks.append({
                    "chunk_id": chunk.id,
                    "document_id": chunk.document_id,
                    "content": chunk.content,
                    "score": result["score"],
                    "metadata": chunk.metadata
                })

        return relevant_chunks

    async def answer_question(
        self,
        session: AsyncSession,
        question: str,
        qa_session_id: int
    ) -> Dict[str, Any]:
        """Answer a question using RAG."""
        # Retrieve relevant chunks
        relevant_chunks = await self.retrieve_relevant_chunks(session, question, qa_session_id)

        if not relevant_chunks:
            return {
                "question": question,
                "answer": "I couldn't find any relevant information to answer your question.",
                "sources": []
            }

        # Prepare context from relevant chunks
        context = "\n\n".join([f"Document {chunk['document_id']}, Chunk {chunk['chunk_id']}: {chunk['content']}"
                              for chunk in relevant_chunks])

        # In a real application, you would use a real LLM here
        # For this example, we'll use a simple template-based approach
        prompt = PromptTemplate(
            template=self.qa_template,
            input_variables=["context", "question"]
        )

        chain = LLMChain(llm=self.llm, prompt=prompt)

        # Generate answer (in a real app, this would call the LLM)
        # For demo purposes, we'll create a mock answer
        answer = f"Based on the retrieved documents, I can provide the following answer: "
        answer += f"This is a simulated answer that would be generated by an LLM based on the {len(relevant_chunks)} most relevant chunks."

        # Store the question and answer
        new_question = Question(
            qa_session_id=qa_session_id,
            question_text=question,
            answer_text=answer,
            retrieval_metadata={
                "chunks": [
                    {
                        "chunk_id": chunk["chunk_id"],
                        "document_id": chunk["document_id"],
                        "score": chunk["score"]
                    } for chunk in relevant_chunks
                ]
            }
        )
        session.add(new_question)
        await session.commit()

        # Prepare sources for the response
        sources = []
        for chunk in relevant_chunks:
            # Get the document title
            document = await session.get(Document, chunk["document_id"])
            sources.append({
                "document_id": chunk["document_id"],
                "document_title": document.title if document else "Unknown",
                "chunk_id": chunk["chunk_id"],
                "score": chunk["score"],
                "content_preview": chunk["content"][:200] + "..." if len(chunk["content"]) > 200 else chunk["content"]
            })

        return {
            "question": question,
            "answer": answer,
            "sources": sources
        }
